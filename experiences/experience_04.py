# experience_04.py

import tensorflow as tf
from tensorflow import keras
from keras import layers
import pandas as pd
from load_pet_dataset import load_pet_dataset

# âš™ï¸ ParamÃ¨tres
image_size = (180, 180)
batch_size = 32
epochs = 5

# ğŸ“¦ Dataset Cats vs Dogs
train_ds, val_ds = load_pet_dataset(image_size=image_size, batch_size=batch_size)

# ğŸ” Charger le modÃ¨le prÃ©entraÃ®nÃ© sur Stanford Dogs
base_model = keras.models.load_model("../stanford_dogs/stanford_dogs_model.keras")

# â„ï¸ On gÃ¨le tous les poids
for layer in base_model.layers:
    layer.trainable = False

# ğŸ§© On rÃ©cupÃ¨re toutes les couches sauf les deux derniÃ¨res convolutives + dense
# Ici, on garde jusqu'Ã  lâ€™avant-avant derniÃ¨re grosse conv (728), on coupe avant les couches finales

# Trouve l'index de la couche Ã  laquelle couper si besoin :
cut_index = -6  # empirique si tu veux personnaliser

inputs = keras.Input(shape=image_size + (3,))
x = layers.Rescaling(1. / 255)(inputs)

# On rejoue les premiÃ¨res couches de base_model sauf les deux derniÃ¨res convolutives
for layer in base_model.layers[2:cut_index]:
    x = layer(x)

# On remplace les deux derniÃ¨res grosses convolutions
x = layers.SeparableConv2D(1024, 3, padding="same", activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.25)(x)
x = layers.Dense(1)(x)  # logits

model = keras.Model(inputs, x)

# âœ… Compilation
model.compile(
    optimizer=keras.optimizers.Adam(1e-4),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
    metrics=["accuracy"]
)

# ğŸš€ EntraÃ®nement
history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)
model.save("../PetImages/Models/cats_vs_dogs_transfer_exp4.keras")
pd.DataFrame(history.history).to_csv("../PetImages/Models/training_log_ex4.csv")

print("âœ… ExpÃ©rience 4 terminÃ©e.")
